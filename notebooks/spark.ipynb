{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-25 17:10:44--  https://jdbc.postgresql.org/download/postgresql-42.7.4.jar\n",
      "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
      "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1086687 (1.0M) [application/java-archive]\n",
      "Saving to: ‘/home/jovyan/work/postgresql-42.7.4.jar.1’\n",
      "\n",
      "postgresql-42.7.4.j 100%[===================>]   1.04M   732KB/s    in 1.4s    \n",
      "\n",
      "2024-12-25 17:10:47 (732 KB/s) - ‘/home/jovyan/work/postgresql-42.7.4.jar.1’ saved [1086687/1086687]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://jdbc.postgresql.org/download/postgresql-42.7.4.jar -P /home/jovyan/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spark Session Info ===\n",
      "[('spark.master', 'local'), ('spark.app.submitTime', '1735146539698'), ('spark.executor.id', 'driver'), ('spark.driver.host', 'ee3eb0a478a1'), ('spark.app.startTime', '1735146539776'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.name', 'SparkConnectionTest'), ('spark.jars', 'postgresql-42.7.4.jar'), ('spark.app.id', 'local-1735146540155'), ('spark.rdd.compress', 'True'), ('spark.app.initial.jar.urls', 'spark://ee3eb0a478a1:36161/jars/postgresql-42.7.4.jar'), ('spark.serializer.objectStreamReset', '100'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.driver.port', '36161'), ('spark.ui.showConsoleProgress', 'true'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.repl.local.jars', 'file:///home/jovyan/work/postgresql-42.7.4.jar')]\n",
      "PostgreSQL Connection Successful!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def buildSpark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"SparkConnectionTest\") \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.7.4.jar\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    #Проверка соединения со Spark\n",
    "    print(\"=== Spark Session Info ===\")\n",
    "    print(spark.sparkContext.getConf().getAll())\n",
    "    return spark\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres_db:5432/mydatabase\"\n",
    "db_properties = {\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "def runSQL(spark, command):\n",
    "    try:\n",
    "        df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=command,  # Тестовый запрос\n",
    "            properties=db_properties\n",
    "        )\n",
    "        print(\"PostgreSQL Connection Successful!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"PostgreSQL Connection Failed! If you just downloaded postgree driver, restart docker compose services please.\")\n",
    "\n",
    "# Настройка соединения со Spark Master\n",
    "spark = buildSpark()\n",
    "# Проверка соединения с PostgreSQL\n",
    "runSQL(spark, \"(SELECT 1) as test\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into DWH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading craftsmans dimension (incremental)...\n",
      "No new craftsmans found. Skipping write to DWH.\n",
      "Loading products dimension (incremental)...\n",
      "No new products found. Skipping write to DWH.\n",
      "Loading customers dimension (incremental)...\n",
      "No new customers found. Skipping write to DWH.\n",
      "Loading orders fact table (incremental)...\n",
      "+----------+------------+-----------+------------------+------------+---------------------+---------+\n",
      "|product_id|craftsman_id|customer_id|order_created_date|order_status|order_completion_date|load_dttm|\n",
      "+----------+------------+-----------+------------------+------------+---------------------+---------+\n",
      "+----------+------------+-----------+------------------+------------+---------------------+---------+\n",
      "\n",
      "Found 0 new records for f_orders (incremental).\n",
      "No new orders to insert. Skipping write.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, col, monotonically_increasing_id\n",
    "\n",
    "def buildSpark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"CraftMarketDWH\") \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.7.4.jar\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# Database connection parameters\n",
    "jdbc_url = \"jdbc:postgresql://postgres_db:5432/mydatabase\"\n",
    "db_properties = {\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "def read_table(schema, table_name):\n",
    "    \"\"\"Helper function to read tables from PostgreSQL\"\"\"\n",
    "    return spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=f\"{schema}.{table_name}\",\n",
    "        properties=db_properties\n",
    "    )\n",
    "\n",
    "def execute_sql(sql):\n",
    "    \"\"\"Execute SQL command directly\"\"\"\n",
    "    import psycopg2\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"mydatabase\",\n",
    "        user=\"myuser\",\n",
    "        password=\"mypassword\",\n",
    "        host=\"postgres_db\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def clean_dwh():\n",
    "    \"\"\"Clean up DWH tables in correct order\"\"\"\n",
    "    try:\n",
    "        # Delete in proper order due to FK constraints\n",
    "        execute_sql(\"DELETE FROM dwh.f_orders;\")\n",
    "        execute_sql(\"DELETE FROM dwh.d_craftsmans;\")\n",
    "        execute_sql(\"DELETE FROM dwh.d_products;\")\n",
    "        execute_sql(\"DELETE FROM dwh.d_customers;\")\n",
    "        print(\"DWH tables cleaned successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning DWH tables: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def write_to_dwh(df, table_name):\n",
    "    \"\"\"Helper function to write dataframes to DWH\"\"\"\n",
    "    df.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(url=jdbc_url, table=table_name, properties=db_properties)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = buildSpark()\n",
    "\n",
    "def load_craftsmans_incremental():\n",
    "    \"\"\"\n",
    "    Incrementally load dwh.d_craftsmans, detecting new records by 'craftsman_email'.\n",
    "    If a record with the same email is already present in DWH, we skip it.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "    print(\"Loading craftsmans dimension (incremental)...\")\n",
    "    \n",
    "    # --- 1) Read existing DWH dimension (to skip duplicates) ---\n",
    "    try:\n",
    "        existing_dwh_crafts = read_table(\"dwh\", \"d_craftsmans\").select(\"craftsman_email\").distinct()\n",
    "    except:\n",
    "        # If table doesn't exist or is empty, create an empty DF with the same schema\n",
    "        existing_dwh_crafts = spark.createDataFrame([], \"craftsman_email STRING\")\n",
    "\n",
    "    # --- 2) Read from the three source tables ---\n",
    "    craft_wide = read_table(\"source1\", \"craft_market_wide\").select(\n",
    "        \"craftsman_id\",\n",
    "        \"craftsman_name\",\n",
    "        \"craftsman_address\",\n",
    "        \"craftsman_birthday\",\n",
    "        \"craftsman_email\"\n",
    "    )\n",
    "\n",
    "    craft_masters = read_table(\"source2\", \"craft_market_masters_products\").select(\n",
    "        \"craftsman_id\",\n",
    "        \"craftsman_name\",\n",
    "        \"craftsman_address\",\n",
    "        \"craftsman_birthday\",\n",
    "        \"craftsman_email\"\n",
    "    )\n",
    "\n",
    "    craft_source3 = read_table(\"source3\", \"craft_market_craftsmans\").select(\n",
    "        \"craftsman_id\",\n",
    "        \"craftsman_name\",\n",
    "        \"craftsman_address\",\n",
    "        \"craftsman_birthday\",\n",
    "        \"craftsman_email\"\n",
    "    )\n",
    "\n",
    "    # Union all new source data\n",
    "    all_craftsmans_src = craft_wide.union(craft_masters).union(craft_source3) \\\n",
    "        .dropDuplicates([\"craftsman_email\"])  # business key dedup\n",
    "\n",
    "    # --- 3) Identify *new* craftsmans: left_anti join by email ---\n",
    "    new_craftsmans = all_craftsmans_src.join(existing_dwh_crafts, \n",
    "                                             on=[\"craftsman_email\"], \n",
    "                                             how=\"left_anti\") \\\n",
    "                                       .withColumn(\"load_dttm\", current_timestamp())\n",
    "\n",
    "    # If there's nothing new, we skip writing:\n",
    "    count_new = new_craftsmans.count()\n",
    "    if count_new == 0:\n",
    "        print(\"No new craftsmans found. Skipping write to DWH.\")\n",
    "        return None\n",
    "    \n",
    "    # --- 4) Write only new records to DWH ---\n",
    "    write_to_dwh(new_craftsmans.drop(\"craftsman_id\"), \"dwh.d_craftsmans\")  # dropping source PK if you wish\n",
    "    print(f\"Inserted {count_new} new craftsmans into DWH.\")\n",
    "    return new_craftsmans\n",
    "\n",
    "\n",
    "def load_customers_incremental():\n",
    "    \"\"\"\n",
    "    Incrementally load dwh.d_customers, detecting new records by 'customer_email'.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "    print(\"Loading customers dimension (incremental)...\")\n",
    "\n",
    "    # --- 1) Read existing DWH dimension ---\n",
    "    try:\n",
    "        existing_dwh_customers = read_table(\"dwh\", \"d_customers\").select(\"customer_email\").distinct()\n",
    "    except:\n",
    "        existing_dwh_customers = spark.createDataFrame([], \"customer_email STRING\")\n",
    "\n",
    "    # --- 2) Read from the three source tables ---\n",
    "    customers_wide = read_table(\"source1\", \"craft_market_wide\").select(\n",
    "        \"customer_name\",\n",
    "        \"customer_address\",\n",
    "        \"customer_birthday\",\n",
    "        \"customer_email\"\n",
    "    )\n",
    "\n",
    "    customers_source2 = read_table(\"source2\", \"craft_market_orders_customers\").select(\n",
    "        \"customer_name\",\n",
    "        \"customer_address\",\n",
    "        \"customer_birthday\",\n",
    "        \"customer_email\"\n",
    "    )\n",
    "\n",
    "    customers_source3 = read_table(\"source3\", \"craft_market_customers\").select(\n",
    "        \"customer_name\",\n",
    "        \"customer_address\",\n",
    "        \"customer_birthday\",\n",
    "        \"customer_email\"\n",
    "    )\n",
    "\n",
    "    all_customers_src = customers_wide.union(customers_source2).union(customers_source3) \\\n",
    "        .dropDuplicates([\"customer_email\"])\n",
    "\n",
    "    # --- 3) left_anti join to find truly new customers ---\n",
    "    new_customers = all_customers_src.join(existing_dwh_customers, \n",
    "                                           on=[\"customer_email\"], \n",
    "                                           how=\"left_anti\") \\\n",
    "                                     .withColumn(\"load_dttm\", current_timestamp())\n",
    "\n",
    "    count_new = new_customers.count()\n",
    "    if count_new == 0:\n",
    "        print(\"No new customers found. Skipping write to DWH.\")\n",
    "        return None\n",
    "\n",
    "    # --- 4) Write only new records to DWH ---\n",
    "    write_to_dwh(new_customers, \"dwh.d_customers\")\n",
    "    print(f\"Inserted {count_new} new customers into DWH.\")\n",
    "    return new_customers\n",
    "\n",
    "\n",
    "def load_products_incremental():\n",
    "    \"\"\"\n",
    "    Incrementally load dwh.d_products, detecting new records by (product_name, product_description).\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "    print(\"Loading products dimension (incremental)...\")\n",
    "\n",
    "    # --- 1) Read existing dwh dimension ---\n",
    "    try:\n",
    "        existing_dwh_products = read_table(\"dwh\", \"d_products\") \\\n",
    "            .select(\"product_name\", \"product_description\") \\\n",
    "            .distinct()\n",
    "    except:\n",
    "        existing_dwh_products = spark.createDataFrame([], \"product_name STRING, product_description STRING\")\n",
    "\n",
    "    # --- 2) Read source tables ---\n",
    "    products_wide = read_table(\"source1\", \"craft_market_wide\").select(\n",
    "        \"product_name\",\n",
    "        \"product_description\",\n",
    "        \"product_type\",\n",
    "        \"product_price\"\n",
    "    )\n",
    "\n",
    "    products_masters = read_table(\"source2\", \"craft_market_masters_products\").select(\n",
    "        \"product_name\",\n",
    "        \"product_description\",\n",
    "        \"product_type\",\n",
    "        \"product_price\"\n",
    "    )\n",
    "\n",
    "    products_orders = read_table(\"source3\", \"craft_market_orders\").select(\n",
    "        \"product_name\",\n",
    "        \"product_description\",\n",
    "        \"product_type\",\n",
    "        \"product_price\"\n",
    "    )\n",
    "\n",
    "    all_products_src = products_wide.union(products_masters).union(products_orders) \\\n",
    "        .dropDuplicates([\"product_name\", \"product_description\"])\n",
    "\n",
    "    # --- 3) left_anti to find truly new products ---\n",
    "    new_products = all_products_src.join(existing_dwh_products,\n",
    "                                         on=[\"product_name\", \"product_description\"],\n",
    "                                         how=\"left_anti\") \\\n",
    "                                   .withColumn(\"load_dttm\", current_timestamp())\n",
    "\n",
    "    count_new = new_products.count()\n",
    "    if count_new == 0:\n",
    "        print(\"No new products found. Skipping write to DWH.\")\n",
    "        return None\n",
    "\n",
    "    # --- 4) Write new records to DWH ---\n",
    "    write_to_dwh(new_products, \"dwh.d_products\")\n",
    "    print(f\"Inserted {count_new} new products into DWH.\")\n",
    "    return new_products\n",
    "\n",
    "\n",
    "def load_orders_incremental():\n",
    "    \"\"\"\n",
    "    Incrementally load f_orders by detecting new records based on \n",
    "    (product_id, craftsman_id, customer_id, order_created_date, order_completion_date, order_status).\n",
    "    Because there's no natural 'order_id', we treat that combo as the unique key. \n",
    "    If a row with that exact combo is already in f_orders, we skip it.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "    print(\"Loading orders fact table (incremental)...\")\n",
    "\n",
    "    # 1) Read dimension tables (already loaded)\n",
    "    dwh_craftsmans = read_table(\"dwh\", \"d_craftsmans\")\n",
    "    dwh_customers = read_table(\"dwh\", \"d_customers\")\n",
    "    dwh_products = read_table(\"dwh\", \"d_products\")\n",
    "\n",
    "    # 2) Read existing f_orders to skip duplicates\n",
    "    try:\n",
    "        existing_orders = read_table(\"dwh\", \"f_orders\") \\\n",
    "            .select(\"product_id\", \"craftsman_id\", \"customer_id\",\n",
    "                    \"order_created_date\", \"order_completion_date\", \"order_status\") \\\n",
    "            .distinct()\n",
    "    except:\n",
    "        existing_orders = spark.createDataFrame([], \n",
    "            \"product_id LONG, craftsman_id LONG, customer_id LONG, order_created_date TIMESTAMP, order_completion_date TIMESTAMP, order_status STRING\"\n",
    "        )\n",
    "\n",
    "    # --- Prepare Source1 Orders ---\n",
    "    orders_source1 = read_table(\"source1\", \"craft_market_wide\").select(\n",
    "        \"craftsman_email\",\n",
    "        \"customer_email\",\n",
    "        \"product_name\",\n",
    "        \"order_created_date\",\n",
    "        \"order_completion_date\",\n",
    "        \"order_status\"\n",
    "    ).distinct()\n",
    "\n",
    "    processed_orders1 = orders_source1 \\\n",
    "        .join(dwh_craftsmans, [\"craftsman_email\"]) \\\n",
    "        .join(dwh_customers, [\"customer_email\"]) \\\n",
    "        .join(dwh_products, [\"product_name\"]) \\\n",
    "        .select(\n",
    "            col(\"product_id\"),\n",
    "            col(\"craftsman_id\"),\n",
    "            col(\"customer_id\"),\n",
    "            col(\"order_created_date\"),\n",
    "            col(\"order_completion_date\"),\n",
    "            col(\"order_status\")\n",
    "        )\n",
    "\n",
    "    # --- Prepare Source2 Orders ---\n",
    "    orders_source2 = read_table(\"source2\", \"craft_market_orders_customers\")\n",
    "    masters_source2 = read_table(\"source2\", \"craft_market_masters_products\")\n",
    "\n",
    "    orders_source2_with_masters = orders_source2.join(\n",
    "        masters_source2, [\"craftsman_id\", \"product_id\"]\n",
    "    )\n",
    "\n",
    "    processed_orders2 = orders_source2_with_masters \\\n",
    "        .join(dwh_craftsmans, orders_source2_with_masters.craftsman_email == dwh_craftsmans.craftsman_email) \\\n",
    "        .join(dwh_customers, orders_source2_with_masters.customer_email == dwh_customers.customer_email) \\\n",
    "        .join(dwh_products, orders_source2_with_masters.product_name == dwh_products.product_name) \\\n",
    "        .select(\n",
    "            dwh_products.product_id,\n",
    "            dwh_craftsmans.craftsman_id,\n",
    "            dwh_customers.customer_id,\n",
    "            col(\"order_created_date\"),\n",
    "            col(\"order_completion_date\"),\n",
    "            col(\"order_status\")\n",
    "        )\n",
    "\n",
    "    # --- Prepare Source3 Orders ---\n",
    "    orders_source3 = read_table(\"source3\", \"craft_market_orders\")\n",
    "    craftsmans_source3 = read_table(\"source3\", \"craft_market_craftsmans\")\n",
    "    customers_source3 = read_table(\"source3\", \"craft_market_customers\")\n",
    "\n",
    "    orders_source3_enriched = orders_source3 \\\n",
    "        .join(craftsmans_source3, [\"craftsman_id\"]) \\\n",
    "        .join(customers_source3, [\"customer_id\"])\n",
    "\n",
    "    processed_orders3 = orders_source3_enriched \\\n",
    "        .join(dwh_craftsmans, orders_source3_enriched.craftsman_email == dwh_craftsmans.craftsman_email) \\\n",
    "        .join(dwh_customers, orders_source3_enriched.customer_email == dwh_customers.customer_email) \\\n",
    "        .join(dwh_products, orders_source3_enriched.product_name == dwh_products.product_name) \\\n",
    "        .select(\n",
    "            dwh_products.product_id,\n",
    "            dwh_craftsmans.craftsman_id,\n",
    "            dwh_customers.customer_id,\n",
    "            col(\"order_created_date\"),\n",
    "            col(\"order_completion_date\"),\n",
    "            col(\"order_status\")\n",
    "        )\n",
    "\n",
    "    # --- Combine all processed orders from the 3 sources ---\n",
    "    all_orders = processed_orders1.union(processed_orders2).union(processed_orders3) \\\n",
    "        .distinct() \\\n",
    "        .withColumn(\"load_dttm\", current_timestamp())\n",
    "\n",
    "    # --- Identify truly NEW orders by left_anti on the unique columns ---\n",
    "    new_orders = all_orders.join(\n",
    "        existing_orders,\n",
    "        on=[\n",
    "            \"product_id\", \n",
    "            \"craftsman_id\", \n",
    "            \"customer_id\", \n",
    "            \"order_created_date\", \n",
    "            # \"order_completion_date\",\n",
    "            \"order_status\"\n",
    "        ],\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "    new_orders.show(10, truncate=False)\n",
    "\n",
    "    count_new = new_orders.count()\n",
    "    print(f\"Found {count_new} new records for f_orders (incremental).\")\n",
    "\n",
    "    if count_new == 0:\n",
    "        print(\"No new orders to insert. Skipping write.\")\n",
    "        return None\n",
    "\n",
    "    # --- Write only new orders to f_orders ---\n",
    "    write_to_dwh(new_orders, \"dwh.f_orders\")\n",
    "    print(\"Orders fact table loaded successfully (incremental).\")\n",
    "\n",
    "    return new_orders\n",
    "\n",
    "## Usage function\n",
    "def loadDataIntoDWH():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"CraftMarketDatamart\") \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.7.4.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    jdbc_url = \"jdbc:postgresql://postgres_db:5432/mydatabase\"\n",
    "    db_properties = {\n",
    "        \"user\": \"myuser\",\n",
    "        \"password\": \"mypassword\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    craftsmans_df = load_craftsmans_incremental()\n",
    "    products_df   = load_products_incremental()\n",
    "    customers_df  = load_customers_incremental()\n",
    "    orders_df = load_orders_incremental()\n",
    "\n",
    "clean_dwh() # это первый раз, почистим чтобы не было ничего\n",
    "\n",
    "craftsmans_df = load_craftsmans_incremental()\n",
    "products_df   = load_products_incremental()\n",
    "customers_df  = load_customers_incremental()\n",
    "orders_df = load_orders_incremental()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data verification...\n",
      "--------------------------------------------------\n",
      "\n",
      "Verifying Craftsmen counts:\n",
      "Source1 unique craftsmen: 997\n",
      "Source2 unique craftsmen: 999\n",
      "Source3 unique craftsmen: 1002\n",
      "DWH craftsmen: 2998\n",
      "\n",
      "Verifying Products counts:\n",
      "Source1 unique products: 908\n",
      "Source2 unique products: 903\n",
      "Source3 unique products: 947\n",
      "DWH products: 2480\n",
      "\n",
      "Verifying Customers counts:\n",
      "Source1 unique customers: 999\n",
      "Source2 unique customers: 999\n",
      "Source3 unique customers: 1000\n",
      "DWH customers: 2998\n",
      "\n",
      "Verifying Orders counts:\n",
      "Source1 unique orders: 999\n",
      "Source2 unique orders: 999\n",
      "Source3 unique orders: 1013\n",
      "DWH orders: 3549\n",
      "\n",
      "Verification complete!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct, col\n",
    "\n",
    "def verify_data_consistency():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"DataVerification\") \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.7.4.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Database connection parameters\n",
    "    jdbc_url = \"jdbc:postgresql://postgres_db:5432/mydatabase\"\n",
    "    db_properties = {\n",
    "        \"user\": \"myuser\",\n",
    "        \"password\": \"mypassword\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    def read_table(schema, table_name):\n",
    "        return spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=f\"{schema}.{table_name}\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "\n",
    "    print(\"Starting data verification...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Verify Craftsmen\n",
    "    print(\"\\nVerifying Craftsmen counts:\")\n",
    "    \n",
    "    # Source counts\n",
    "    craft_wide = read_table(\"source1\", \"craft_market_wide\").select(\"craftsman_email\").distinct().count()\n",
    "    craft_masters = read_table(\"source2\", \"craft_market_masters_products\").select(\"craftsman_email\").distinct().count()\n",
    "    craft_source3 = read_table(\"source3\", \"craft_market_craftsmans\").select(\"craftsman_email\").distinct().count()\n",
    "    \n",
    "    # DWH count\n",
    "    dwh_craftsmen = read_table(\"dwh\", \"d_craftsmans\").count()\n",
    "    \n",
    "    print(f\"Source1 unique craftsmen: {craft_wide}\")\n",
    "    print(f\"Source2 unique craftsmen: {craft_masters}\")\n",
    "    print(f\"Source3 unique craftsmen: {craft_source3}\")\n",
    "    print(f\"DWH craftsmen: {dwh_craftsmen}\")\n",
    "\n",
    "    # Verify Products\n",
    "    print(\"\\nVerifying Products counts:\")\n",
    "    \n",
    "    # Source counts\n",
    "    products_wide = read_table(\"source1\", \"craft_market_wide\").select(\"product_name\", \"product_type\").distinct().count()\n",
    "    products_masters = read_table(\"source2\", \"craft_market_masters_products\").select(\"product_name\", \"product_type\").distinct().count()\n",
    "    products_orders = read_table(\"source3\", \"craft_market_orders\").select(\"product_name\", \"product_type\").distinct().count()\n",
    "    \n",
    "    # DWH count\n",
    "    dwh_products = read_table(\"dwh\", \"d_products\").count()\n",
    "    \n",
    "    print(f\"Source1 unique products: {products_wide}\")\n",
    "    print(f\"Source2 unique products: {products_masters}\")\n",
    "    print(f\"Source3 unique products: {products_orders}\")\n",
    "    print(f\"DWH products: {dwh_products}\")\n",
    "\n",
    "    # Verify Customers\n",
    "    print(\"\\nVerifying Customers counts:\")\n",
    "    \n",
    "    # Source counts\n",
    "    customers_wide = read_table(\"source1\", \"craft_market_wide\").select(\"customer_email\").distinct().count()\n",
    "    customers_source2 = read_table(\"source2\", \"craft_market_orders_customers\").select(\"customer_email\").distinct().count()\n",
    "    customers_source3 = read_table(\"source3\", \"craft_market_customers\").select(\"customer_email\").distinct().count()\n",
    "    \n",
    "    # DWH count\n",
    "    dwh_customers = read_table(\"dwh\", \"d_customers\").count()\n",
    "    \n",
    "    print(f\"Source1 unique customers: {customers_wide}\")\n",
    "    print(f\"Source2 unique customers: {customers_source2}\")\n",
    "    print(f\"Source3 unique customers: {customers_source3}\")\n",
    "    print(f\"DWH customers: {dwh_customers}\")\n",
    "\n",
    "    # Verify Orders\n",
    "    print(\"\\nVerifying Orders counts:\")\n",
    "    \n",
    "    # Source counts\n",
    "    orders_wide = read_table(\"source1\", \"craft_market_wide\").select(\n",
    "        \"order_created_date\", \"order_completion_date\", \"order_status\",\n",
    "        \"craftsman_email\", \"customer_email\", \"product_name\"\n",
    "    ).distinct().count()\n",
    "    \n",
    "    orders_source2 = read_table(\"source2\", \"craft_market_orders_customers\").select(\n",
    "        \"order_created_date\", \"order_completion_date\", \"order_status\",\n",
    "        \"craftsman_id\", \"customer_id\", \"product_id\"\n",
    "    ).distinct().count()\n",
    "    \n",
    "    orders_source3 = read_table(\"source3\", \"craft_market_orders\").select(\n",
    "        \"order_created_date\", \"order_completion_date\", \"order_status\",\n",
    "        \"craftsman_id\", \"customer_id\", \"product_id\"\n",
    "    ).distinct().count()\n",
    "    \n",
    "    # DWH count\n",
    "    dwh_orders = read_table(\"dwh\", \"f_orders\").count()\n",
    "    \n",
    "    print(f\"Source1 unique orders: {orders_wide}\")\n",
    "    print(f\"Source2 unique orders: {orders_source2}\")\n",
    "    print(f\"Source3 unique orders: {orders_source3}\")\n",
    "    print(f\"DWH orders: {dwh_orders}\")\n",
    "\n",
    "    print(\"\\nVerification complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verify_data_consistency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting data into DataMART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, month, year, concat, lit, count, avg, sum, datediff, \n",
    "    percentile_approx, collect_list, struct, first, when, current_date\n",
    ")\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_info(message):\n",
    "    \"\"\"Helper function to both log and print messages\"\"\"\n",
    "    print(f\"INFO: {message}\")\n",
    "    logger.info(message)\n",
    "\n",
    "class DatamartLoader:\n",
    "    def __init__(self, spark, jdbc_url, db_properties):\n",
    "        self.spark = spark\n",
    "        self.jdbc_url = jdbc_url\n",
    "        self.db_properties = db_properties\n",
    "\n",
    "    def read_table(self, schema, table_name):\n",
    "        \"\"\"Helper function to read tables from PostgreSQL\"\"\"\n",
    "        return self.spark.read.jdbc(\n",
    "            url=self.jdbc_url,\n",
    "            table=f\"{schema}.{table_name}\",\n",
    "            properties=self.db_properties\n",
    "        )\n",
    "\n",
    "    def write_to_table(self, df, table_name, mode=\"append\"):\n",
    "        \"\"\"Helper function to write dataframes to PostgreSQL\"\"\"\n",
    "        df.write.jdbc(\n",
    "            url=self.jdbc_url,\n",
    "            table=table_name,\n",
    "            mode=mode,\n",
    "            properties=self.db_properties\n",
    "        )\n",
    "\n",
    "    def get_last_load_date(self):\n",
    "        \"\"\"\n",
    "        Get the last load date (TIMESTAMP) from the tracking table.\n",
    "        Returns None if no record exists.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            load_dates_df = self.read_table(\"dwh\", \"load_dates_craftsman_report_datamart\")\n",
    "            count_records = load_dates_df.count()\n",
    "            log_info(f\"Found {count_records} records in load dates tracking table\")\n",
    "            \n",
    "            if count_records == 0:\n",
    "                log_info(\"No previous load date found (empty table).\")\n",
    "                return None\n",
    "            \n",
    "            last_date = load_dates_df.orderBy(col(\"load_dttm\").desc()).first()[\"load_dttm\"]\n",
    "            log_info(f\"Last load date from tracking table: {last_date}\")\n",
    "            return last_date\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_info(f\"Error reading load dates: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def update_load_date(self):\n",
    "        \"\"\"Update the load tracking table with the current max load_dttm from f_orders.\"\"\"\n",
    "        log_info(\"Updating load date tracking table...\")\n",
    "        orders = self.read_table(\"dwh\", \"f_orders\")\n",
    "        \n",
    "        max_load_date = orders.agg({\"load_dttm\": \"max\"}).collect()[0][0]\n",
    "        \n",
    "        if max_load_date:\n",
    "            log_info(f\"Recording max order load date: {max_load_date}\")\n",
    "            current_date_df = self.spark.createDataFrame(\n",
    "                [(max_load_date,)], [\"load_dttm\"]\n",
    "            )\n",
    "            self.write_to_table(current_date_df, \"dwh.load_dates_craftsman_report_datamart\")\n",
    "            log_info(\"Load date tracking table updated successfully\")\n",
    "        else:\n",
    "            log_info(\"No orders found, skipping load date update.\")\n",
    "\n",
    "    def calculate_datamart_metrics(self, last_load_date=None):\n",
    "        \"\"\"Calculate all metrics for the datamart, using incremental filtering on load_dttm.\"\"\"\n",
    "        log_info(\"Starting datamart metrics calculation...\")\n",
    "        \n",
    "        # Read dimension and fact tables\n",
    "        log_info(\"Reading dimension tables: d_craftsmans, d_customers, d_products...\")\n",
    "        craftsmen = self.read_table(\"dwh\", \"d_craftsmans\")\n",
    "        customers = self.read_table(\"dwh\", \"d_customers\")\n",
    "        products = self.read_table(\"dwh\", \"d_products\")\n",
    "        \n",
    "        log_info(\"Reading orders table: f_orders...\")\n",
    "        orders = self.read_table(\"dwh\", \"f_orders\")\n",
    "\n",
    "        total_orders = orders.count()\n",
    "        log_info(f\"Total orders in f_orders (unfiltered): {total_orders}\")\n",
    "        \n",
    "        min_date = orders.agg({\"load_dttm\": \"min\"}).collect()[0][0]\n",
    "        max_date = orders.agg({\"load_dttm\": \"max\"}).collect()[0][0]\n",
    "        log_info(f\"Orders load_dttm range: {min_date} to {max_date}\")\n",
    "        \n",
    "        # If we have a last_load_date, we do incremental filtering\n",
    "        if last_load_date:\n",
    "            log_info(f\"Filtering orders with load_dttm > {last_load_date}\")\n",
    "            orders = orders.filter(col(\"load_dttm\") > last_load_date)\n",
    "            \n",
    "            filtered_count = orders.count()\n",
    "            log_info(f\"After filtering, {filtered_count} orders remain.\")\n",
    "        else:\n",
    "            log_info(\"No last load date found, so we process ALL orders.\")\n",
    "            filtered_count = orders.count()\n",
    "        \n",
    "        if filtered_count == 0:\n",
    "            log_info(\"No new orders to process after filtering.\")\n",
    "            return None\n",
    "        \n",
    "        log_info(f\"Proceeding with processing of {filtered_count} orders...\")\n",
    "\n",
    "        # Add month-year period to orders\n",
    "        orders = orders.withColumn(\n",
    "            \"report_period\",\n",
    "            concat(year(\"order_created_date\"), lit(\"-\"), month(\"order_created_date\"))\n",
    "        )\n",
    "\n",
    "        # Join all necessary dimension data\n",
    "        enriched_orders = orders \\\n",
    "            .join(craftsmen, \"craftsman_id\") \\\n",
    "            .join(customers, \"customer_id\") \\\n",
    "            .join(products, \"product_id\")\n",
    "\n",
    "        # Aggregate to form the result set\n",
    "        result = enriched_orders.groupBy(\"craftsman_id\", \"report_period\").agg(\n",
    "            first(\"craftsman_name\").alias(\"craftsman_name\"),\n",
    "            first(\"craftsman_address\").alias(\"craftsman_address\"),\n",
    "            first(\"craftsman_birthday\").alias(\"craftsman_birthday\"),\n",
    "            first(\"craftsman_email\").alias(\"craftsman_email\"),\n",
    "            \n",
    "            (sum(\"product_price\") * 0.9).alias(\"craftsman_money\"),\n",
    "            (sum(\"product_price\") * 0.1).cast(\"bigint\").alias(\"platform_money\"),\n",
    "            \n",
    "            count(\"order_created_date\").alias(\"count_order\"),\n",
    "            avg(\"product_price\").alias(\"avg_price_order\"),\n",
    "            \n",
    "            avg(datediff(current_date(), col(\"customer_birthday\")) / 365.25)\n",
    "                .cast(\"decimal(3,1)\")\n",
    "                .alias(\"avg_age_customer\"),\n",
    "            \n",
    "            percentile_approx(\n",
    "                when(\n",
    "                    col(\"order_completion_date\").isNotNull(),\n",
    "                    datediff(\"order_completion_date\", \"order_created_date\")\n",
    "                ),\n",
    "                0.5\n",
    "            ).alias(\"median_time_order_completed\"),\n",
    "            \n",
    "            first(\"product_type\").alias(\"top_product_category\"),\n",
    "            \n",
    "            sum(when(col(\"order_status\") == \"created\", 1).otherwise(0)).alias(\"count_order_created\"),\n",
    "            sum(when(col(\"order_status\") == \"in progress\", 1).otherwise(0)).alias(\"count_order_in_progress\"),\n",
    "            sum(when(col(\"order_status\") == \"delivery\", 1).otherwise(0)).alias(\"count_order_delivery\"),\n",
    "            sum(when(col(\"order_status\") == \"done\", 1).otherwise(0)).alias(\"count_order_done\"),\n",
    "            sum(when(col(\"order_status\") != \"done\", 1).otherwise(0)).alias(\"count_order_not_done\")\n",
    "        )\n",
    "\n",
    "        combos_count = result.count()\n",
    "        log_info(f\"Calculated metrics for {combos_count} craftsman-period combinations.\")\n",
    "        return result\n",
    "\n",
    "    def update_datamart(self):\n",
    "        \"\"\"\n",
    "        Main method to handle incremental datamart updates:\n",
    "          1. Get the last load timestamp\n",
    "          2. Filter new orders\n",
    "          3. Aggregate metrics\n",
    "          4. Compare combos to existing datamart\n",
    "          5. Insert only truly new combos\n",
    "          6. Update the load date tracking table\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Get last load date\n",
    "            last_load_date = self.get_last_load_date()\n",
    "            logger.info(f\"Last load date: {last_load_date}\")\n",
    "\n",
    "            # 2 & 3. Calculate new metrics (aggregations) from new/updated orders\n",
    "            new_metrics = self.calculate_datamart_metrics(last_load_date)\n",
    "            if new_metrics is None:\n",
    "                logger.info(\"No new data to process, returning.\")\n",
    "                return\n",
    "\n",
    "            # 4. Compare combos to existing records in the datamart\n",
    "            existing_periods_df = self.read_table(\"dwh\", \"craftsman_report_datamart\")\n",
    "            existing_periods_df = existing_periods_df.select(\"craftsman_id\", \"report_period\").distinct()\n",
    "\n",
    "            # Left-anti join: find combos that don't exist yet\n",
    "            records_to_update = new_metrics.join(\n",
    "                existing_periods_df,\n",
    "                on=[\"craftsman_id\", \"report_period\"],\n",
    "                how=\"left_anti\"\n",
    "            )\n",
    "\n",
    "            # 5. Insert only those that are truly new combos\n",
    "            num_new_records = records_to_update.count()\n",
    "            logger.info(f\"Found {num_new_records} new craftsman-period combos to insert into datamart.\")\n",
    "\n",
    "            if num_new_records > 0:\n",
    "                self.write_to_table(records_to_update, \"dwh.craftsman_report_datamart\")\n",
    "                logger.info(f\"Inserted {num_new_records} new records into datamart.\")\n",
    "\n",
    "                # 6. Update the load tracking table\n",
    "                self.update_load_date()\n",
    "                logger.info(\"Successfully updated datamart and load date tracking.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error updating datamart: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# ------------------------------\n",
    "# Usage function\n",
    "# ------------------------------\n",
    "def run_datamart_update():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"CraftMarketDatamart\") \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.7.4.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    jdbc_url = \"jdbc:postgresql://postgres_db:5432/mydatabase\"\n",
    "    db_properties = {\n",
    "        \"user\": \"myuser\",\n",
    "        \"password\": \"mypassword\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    loader = DatamartLoader(spark, jdbc_url, db_properties)\n",
    "    loader.update_datamart()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_test_data(productName):\n",
    "    \"\"\"Insert test data to verify incremental loading\"\"\"\n",
    "    try:\n",
    "        log_info(\"Inserting test data into source tables...\")\n",
    "        \n",
    "        import psycopg2\n",
    "        \n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"mydatabase\",\n",
    "            user=\"myuser\",\n",
    "            password=\"mypassword\",\n",
    "            host=\"postgres_db\"\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # First, let's check current max IDs\n",
    "        cur.execute(\"SELECT MAX(craftsman_id) FROM source3.craft_market_craftsmans\")\n",
    "        max_craftsman_id = cur.fetchone()[0] or 0\n",
    "        \n",
    "        cur.execute(\"SELECT MAX(customer_id) FROM source3.craft_market_customers\")\n",
    "        max_customer_id = cur.fetchone()[0] or 0\n",
    "        \n",
    "        cur.execute(\"SELECT MAX(order_id) FROM source3.craft_market_orders\")\n",
    "        max_order_id = cur.fetchone()[0] or 0\n",
    "        \n",
    "        log_info(f\"Current max IDs - Craftsman: {max_craftsman_id}, Customer: {max_customer_id}, Order: {max_order_id}\")\n",
    "        \n",
    "        # Override identity column for test data\n",
    "        cur.execute(\"ALTER TABLE source3.craft_market_craftsmans ALTER COLUMN craftsman_id RESTART WITH %s\", \n",
    "                   (max_craftsman_id + 1,))\n",
    "        cur.execute(\"ALTER TABLE source3.craft_market_customers ALTER COLUMN customer_id RESTART WITH %s\", \n",
    "                   (max_customer_id + 1,))\n",
    "        cur.execute(\"ALTER TABLE source3.craft_market_orders ALTER COLUMN order_id RESTART WITH %s\", \n",
    "                   (max_order_id + 1,))\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO source3.craft_market_craftsmans \n",
    "        (craftsman_name, craftsman_address, craftsman_birthday, craftsman_email)\n",
    "        VALUES ('Test Craftsman New2', '123 Test St', '1990-01-01', 'test_new3@test.com')\n",
    "        RETURNING craftsman_id;\n",
    "        \"\"\")\n",
    "        new_craftsman_id = cur.fetchone()[0]\n",
    "        log_info(f\"Inserted new craftsman with ID: {new_craftsman_id}\")\n",
    "        \n",
    "        # Add new customer\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO source3.craft_market_customers \n",
    "        (customer_name, customer_address, customer_birthday, customer_email)\n",
    "        VALUES ('Test Customer New', '456 Test Ave', '1995-01-01', 'testcustomer_new@test.com')\n",
    "        RETURNING customer_id;\n",
    "        \"\"\")\n",
    "        new_customer_id = cur.fetchone()[0]\n",
    "        log_info(f\"Inserted new customer with ID: {new_customer_id}\")\n",
    "        \n",
    "        # Add new order\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO source3.craft_market_orders \n",
    "        (product_name, product_description, product_type, product_price,\n",
    "         craftsman_id, customer_id, order_created_date, order_status)\n",
    "        VALUES \n",
    "        (%s, %s, %s, %s, %s, %s, CURRENT_DATE, %s)\n",
    "        RETURNING order_id;\n",
    "        \"\"\", (\n",
    "            productName,\n",
    "            'A new test product',\n",
    "            'Test Category',\n",
    "            150,\n",
    "            new_craftsman_id,\n",
    "            new_customer_id,\n",
    "            'created'\n",
    "        ))\n",
    "        new_order_id = cur.fetchone()[0]\n",
    "        log_info(f\"Inserted new order with ID: {new_order_id}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        log_info(\"Test data inserted successfully\")\n",
    "        \n",
    "        # Verify what we inserted\n",
    "        cur.execute(\"\"\"\n",
    "        SELECT o.order_id, o.product_name, c.craftsman_name, cust.customer_name\n",
    "        FROM source3.craft_market_orders o\n",
    "        JOIN source3.craft_market_craftsmans c ON o.craftsman_id = c.craftsman_id\n",
    "        JOIN source3.craft_market_customers cust ON o.customer_id = cust.customer_id\n",
    "        WHERE o.order_id = %s\n",
    "        \"\"\", (new_order_id,))\n",
    "        \n",
    "        result = cur.fetchone()\n",
    "        log_info(f\"Verified new order: {result}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_info(f\"Error inserting test data: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас у нас в соурсах оригинальный набор данных, он же лежит в DWH. Загрузим его в витрину"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Found 8 records in load dates tracking table\n",
      "INFO: Last load date from tracking table: 2024-12-25 19:31:02.776379\n",
      "INFO: Starting datamart metrics calculation...\n",
      "INFO: Reading dimension tables: d_craftsmans, d_customers, d_products...\n",
      "INFO: Reading orders table: f_orders...\n",
      "INFO: Total orders in f_orders (unfiltered): 3549\n",
      "INFO: Orders load_dttm range: 2024-12-25 19:57:05.701979 to 2024-12-25 19:57:05.701979\n",
      "INFO: Filtering orders with load_dttm > 2024-12-25 19:31:02.776379\n",
      "INFO: After filtering, 3549 orders remain.\n",
      "INFO: Proceeding with processing of 3549 orders...\n",
      "INFO: Calculated metrics for 2999 craftsman-period combinations.\n",
      "INFO: Updating load date tracking table...\n",
      "INFO: Recording max order load date: 2024-12-25 19:57:05.701979\n",
      "INFO: Load date tracking table updated successfully\n"
     ]
    }
   ],
   "source": [
    "run_datamart_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "теперь в оригинальный источник добавим новый заказ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Inserting test data into source tables...\n",
      "INFO: Current max IDs - Craftsman: 1013, Customer: 1013, Order: 1013\n",
      "INFO: Inserted new craftsman with ID: 1014\n",
      "INFO: Inserted new customer with ID: 1014\n",
      "INFO: Inserted new order with ID: 1014\n",
      "INFO: Test data inserted successfully\n",
      "INFO: Verified new order: (1014, 'TEST ORDER 4', 'Test Craftsman New2', 'Test Customer New')\n"
     ]
    }
   ],
   "source": [
    " #тут надо убедиться чтобы название было оригинальным, иначе его могут срезать как дубликат\n",
    "insert_test_data(\"TEST ORDER 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "теперь прогоним апдейт оттуда в DWH. Он должен произойти инкрементально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading craftsmans dimension (incremental)...\n",
      "No new craftsmans found. Skipping write to DWH.\n",
      "Loading products dimension (incremental)...\n",
      "Inserted 1 new products into DWH.\n",
      "Loading customers dimension (incremental)...\n",
      "No new customers found. Skipping write to DWH.\n",
      "Loading orders fact table (incremental)...\n",
      "+----------+------------+-----------+------------------+------------+---------------------+--------------------------+\n",
      "|product_id|craftsman_id|customer_id|order_created_date|order_status|order_completion_date|load_dttm                 |\n",
      "+----------+------------+-----------+------------------+------------+---------------------+--------------------------+\n",
      "|91843     |110596      |110638     |2024-12-25        |created     |NULL                 |2024-12-25 19:58:25.454518|\n",
      "+----------+------------+-----------+------------------+------------+---------------------+--------------------------+\n",
      "\n",
      "Found 1 new records for f_orders (incremental).\n",
      "Orders fact table loaded successfully (incremental).\n"
     ]
    }
   ],
   "source": [
    "loadDataIntoDWH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Found 9 records in load dates tracking table\n",
      "INFO: Last load date from tracking table: 2024-12-25 19:57:05.701979\n",
      "INFO: Starting datamart metrics calculation...\n",
      "INFO: Reading dimension tables: d_craftsmans, d_customers, d_products...\n",
      "INFO: Reading orders table: f_orders...\n",
      "INFO: Total orders in f_orders (unfiltered): 3550\n",
      "INFO: Orders load_dttm range: 2024-12-25 19:57:05.701979 to 2024-12-25 19:58:26.243153\n",
      "INFO: Filtering orders with load_dttm > 2024-12-25 19:57:05.701979\n",
      "INFO: After filtering, 1 orders remain.\n",
      "INFO: Proceeding with processing of 1 orders...\n",
      "INFO: Calculated metrics for 1 craftsman-period combinations.\n"
     ]
    }
   ],
   "source": [
    "run_datamart_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
